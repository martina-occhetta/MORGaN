{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm  # Use notebook-friendly tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from argparse import Namespace\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('data')\n",
    "# Import your project modules (adjust the import paths as needed)\n",
    "from main_transductive import pretrain\n",
    "from src.utils import set_random_seed, create_optimizer, WBLogger\n",
    "from src.datasets.data_util import load_dataset, load_processed_graph\n",
    "from src.models import build_model\n",
    "from src.evaluation import node_classification_evaluation\n",
    "from src.utils import build_args, load_best_configs  # if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Configuration Settings\n",
    "# ----------------------\n",
    "# Choose device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "seed = 0\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyperparameters\n",
    "dataset_name    = \"CPDB_cdgps\"  # Replace with your dataset's name\n",
    "max_epoch       = 100           # Total training epochs\n",
    "max_epoch_f     = 200           # For fine tuning\n",
    "num_hidden      = 64\n",
    "num_layers      = 3\n",
    "encoder_type    = \"rgcn\"     # Replace with your encoder type\n",
    "decoder_type    = \"rgcn\"     # Replace with your decoder type\n",
    "replace_rate    = 0.05\n",
    "num_edge_types  = 6\n",
    "in_drop         = 0.2\n",
    "attn_drop       = 0.1\n",
    "mask_rate       = 0.5\n",
    "drop_edge_rate  = 0.0\n",
    "alpha_l         = 3\n",
    "num_heads       = 4\n",
    "activation      = \"prelu\"          \n",
    "optimizer       = \"adam\"             # Adjust as needed (e.g., \"sgd\", \"adam\")\n",
    "loss_fn         = \"sce\"        # Replace with your loss function identifier\n",
    "lr              = 0.01\n",
    "weight_decay    = 1e-3\n",
    "lr_f            = 0.005               # Learning rate for linear evaluation phase\n",
    "weight_decay_f  = 1e-4\n",
    "linear_prob     = False\n",
    "load_model      = False              # Set True to load a checkpoint\n",
    "save_model      = True              # Set True to save your trained model\n",
    "logs            = True              # Set True if you want to use WBLogger\n",
    "use_scheduler   = True              # Set True to use a learning rate scheduler\n",
    "weight_decomposition = {'type': 'basis', 'num_bases': 2}\n",
    "vertical_stacking = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Create a Namespace for Args\n",
    "# ----------------------\n",
    "\n",
    "args = Namespace(\n",
    "    device         = device,\n",
    "    seeds          = [seed],\n",
    "    dataset        = dataset_name,\n",
    "    max_epoch      = max_epoch,\n",
    "    max_epoch_f    = max_epoch_f,\n",
    "    num_hidden     = num_hidden,\n",
    "    num_layers     = num_layers,\n",
    "    encoder        = encoder_type,\n",
    "    decoder        = decoder_type,\n",
    "    activation     = activation,\n",
    "    in_drop        = in_drop,\n",
    "    attn_drop      = attn_drop,\n",
    "    mask_rate      = mask_rate,\n",
    "    drop_edge_rate = drop_edge_rate,\n",
    "    alpha_l        = alpha_l,\n",
    "    num_heads      = num_heads,\n",
    "    weight_decomposition = weight_decomposition,\n",
    "    vertical_stacking = vertical_stacking,\n",
    "    replace_rate   = replace_rate,\n",
    "    num_edge_types = num_edge_types,\n",
    "    optimizer      = optimizer,\n",
    "    loss_fn        = loss_fn,\n",
    "    lr             = lr,\n",
    "    weight_decay   = weight_decay,\n",
    "    lr_f           = lr_f,\n",
    "    weight_decay_f = weight_decay_f,\n",
    "    linear_prob    = linear_prob,\n",
    "    load_model     = load_model,\n",
    "    save_model     = save_model,\n",
    "    logging        = logs,\n",
    "    scheduler      = use_scheduler,\n",
    "    num_features   = 6, # To be set after loading dataset\n",
    "    num_out_heads  = 1,\n",
    "    residual = False,\n",
    "    norm = None,\n",
    "    negative_slope = 0.2,\n",
    "    concat_hidden = False,\n",
    "    return_hidden = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreModel(\n",
       "  (encoder): RGCN(\n",
       "    (rgcn_layers): ModuleList(\n",
       "      (0-2): 3 x RGCNConv(64, 64)\n",
       "    )\n",
       "    (activation): PReLU(num_parameters=1)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (decoder): RGCN(\n",
       "    (rgcn_layers): ModuleList(\n",
       "      (0): RGCNConv(64, 64)\n",
       "    )\n",
       "    (activation): PReLU(num_parameters=1)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (encoder_to_decoder): Linear(in_features=64, out_features=64, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------\n",
    "# Load Dataset and Build Model\n",
    "# ----------------------\n",
    "#graph, (num_features, num_classes) = load_dataset(dataset_name)\n",
    "graph = load_processed_graph(f'../data/real/multidim_graph/6d/{dataset_name}_multiomics.pt')\n",
    "num_features = graph.x.shape[1]\n",
    "num_classes = graph.y.max().item() + 1\n",
    "\n",
    "args.num_features = num_features  # Update the args with the number of features\n",
    "\n",
    "model = build_model(args)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Create Optimizer and Scheduler\n",
    "# ----------------------\n",
    "optimizer = create_optimizer(optimizer, model, lr, weight_decay)\n",
    "\n",
    "scheduler = None\n",
    "if use_scheduler:\n",
    "    # Example: cosine scheduler (you can adjust the function as needed)\n",
    "    scheduler_fn = lambda epoch: (1 + np.cos(epoch * np.pi / max_epoch)) * 0.5\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Setup Logger (Optional)\n",
    "# ----------------------\n",
    "logger = WBLogger(name=\"notebook_training\") if logs else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-21 00:27:06,782 - INFO - start training..\n",
      "# Epoch 99: train_loss: 0.1408: 100%|██████████| 100/100 [00:29<00:00,  3.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# Train the Model\n",
    "# ----------------------\n",
    "# Here, graph.x is assumed to be your feature matrix.\n",
    "# If needed, make sure it's on the same device as your model.\n",
    "model = pretrain(model,\n",
    "                 graph,\n",
    "                 graph.x.to(device),\n",
    "                 optimizer,\n",
    "                 max_epoch,\n",
    "                 device,\n",
    "                 scheduler,\n",
    "                 num_classes,\n",
    "                 lr_f,\n",
    "                 weight_decay_f,\n",
    "                 max_epoch_f,\n",
    "                 linear_prob,\n",
    "                 num_edge_types,\n",
    "                 logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, move model to CPU after training (or keep it on GPU)\n",
    "model = model.cpu()\n",
    "\n",
    "# Optionally save or load the model checkpoint\n",
    "if save_model:\n",
    "    torch.save(model.state_dict(), \"../checkpoints/emb_extraction_model.pt\")\n",
    "if load_model:\n",
    "    model.load_state_dict(torch.load(\"checkpoint.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num parameters for finetuning: 66281\n",
      "Number of positive labels: tensor(196.)\n",
      "Number of negative labels: tensor(195.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Epoch: 199, train_loss: 0.0738, val_loss: 0.8964, val_acc:0.8877551020408163, val_aupr: 0.9635, test_loss: 1.2509, test_acc: 0.9106,test_auc: 0.9322, test_aupr: 0.9200: 100%|██████████| 200/200 [01:07<00:00,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TestAcc: 0.9106, early-stopping-TestAcc: 0.9024, Best ValAcc: 0.8980 in epoch 164 --- \n",
      "--- TestAUPR: 0.9200, early-stopping-TestAUPR: 0.9219, Best ValAUPR: 0.9648 in epoch 197 --- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# Evaluate the Model\n",
    "# ----------------------\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "(test_acc, estp_test_acc), (test_auc, estp_test_auc), (test_aupr, estp_test_aupr), (test_precision, estp_test_precision), (test_recall, estp_test_recall), (test_f1, estp_f1) = node_classification_evaluation(model, graph, graph.x, num_classes, lr_f, weight_decay_f, max_epoch_f, device, linear_prob=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.note({\n",
    "            \"test_accuracy\": test_acc,\n",
    "            \"test_estp_accuracy\": estp_test_acc,\n",
    "            \"test_auc\": test_auc,\n",
    "            \"test_estp_auc\": estp_test_auc,\n",
    "            \"test_aupr\": test_aupr,\n",
    "            \"test_estp_aupr\": estp_test_aupr,\n",
    "            \"test_precision\": test_precision,\n",
    "            \"test_estp_precision\": estp_test_precision,\n",
    "            \"test_recall\": test_recall,\n",
    "            \"test_estp_recall\": estp_test_recall,\n",
    "            \"test_f1\": test_f1,\n",
    "            \"test_estp_f1\": estp_f1\n",
    "            },\n",
    "            step=max_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PreModel.forward() got an unexpected keyword argument 'return_hidden'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Extract embeddings from the model.\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# This assumes your model has a method or a flag to return the embeddings.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# For example, if your forward method can optionally return the latent space:\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Example: model returns a tuple (loss, embeddings) during training,\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# so during evaluation you might modify it to only get embeddings.\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Adjust this part based on your model's implementation.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     final_output, hidden_list = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_edge_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_hidden\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Option 2: Use the hidden representation from the last layer as embeddings\u001b[39;00m\n\u001b[32m     15\u001b[39m embeddings_last_hidden = hidden_list[-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-QueenMary,UniversityofLondon/martina/01_PhD/05_Projects/04_Druggable-genes/SMG-DG/.dg_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-QueenMary,UniversityofLondon/martina/01_PhD/05_Projects/04_Druggable-genes/SMG-DG/.dg_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: PreModel.forward() got an unexpected keyword argument 'return_hidden'"
     ]
    }
   ],
   "source": [
    "# Assume you have already loaded your trained model, graph, and features (x)\n",
    "# Set your model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Extract embeddings from the model.\n",
    "# This assumes your model has a method or a flag to return the embeddings.\n",
    "# For example, if your forward method can optionally return the latent space:\n",
    "with torch.no_grad():\n",
    "    # Example: model returns a tuple (loss, embeddings) during training,\n",
    "    # so during evaluation you might modify it to only get embeddings.\n",
    "    # Adjust this part based on your model's implementation.\n",
    "    final_output, hidden_list = model(graph, graph.x, num_edge_types, return_hidden=True)\n",
    "\n",
    "# Option 2: Use the hidden representation from the last layer as embeddings\n",
    "embeddings_last_hidden = hidden_list[-1]\n",
    "\n",
    "# Convert embeddings to a numpy array if they are a torch.Tensor\n",
    "embeddings_np = embeddings_last_hidden.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply t-SNE to reduce the embedding dimensions to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(embeddings_np)\n",
    "\n",
    "# Plot the embeddings\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=5)\n",
    "plt.title(\"t-SNE Visualization of Latent Embeddings\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
